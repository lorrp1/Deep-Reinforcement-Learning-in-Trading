{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from Environment.gens.TA_Gen import TAStreamer\n",
    "from Environment.envs.indicator_1 import Indicator_1\n",
    "\n",
    "from Agent.duelling_dqn import DDDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def World(filename=None,\n",
    "        train_test = 'train',\n",
    "        episodes=10,\n",
    "        train_test_split = 0.75,\n",
    "        trading_fee = .0001,\n",
    "        time_fee = .001,\n",
    "        memory_size = 3000,\n",
    "        gamma = 0.96,\n",
    "        epsilon_min = 0.01,\n",
    "        batch_size = 64,\n",
    "        train_interval = 10,\n",
    "        learning_rate = 0.001,\n",
    "        render_show=False,\n",
    "        display=False,\n",
    "        save_results=False\n",
    "):\n",
    "    start = time.time()\n",
    "\n",
    "    generator = TAStreamer(filename=filename, mode='train', split=train_test_split)\n",
    "    episode_length = round(int(len(pd.read_csv(filename))*train_test_split), -1)\n",
    "\n",
    "    environment = Indicator_1(data_generator=generator,\n",
    "                              trading_fee=trading_fee,\n",
    "                              time_fee=time_fee,\n",
    "                              episode_length=episode_length)\n",
    "    action_size = len(Indicator_1._actions)\n",
    "\n",
    "    state = environment.reset()\n",
    "\n",
    "    state_size = len(state)\n",
    "\n",
    "\n",
    "    try:\n",
    "        symbol = re.findall(r'Data\\\\([^_]+)',filename)[0]\n",
    "    except:\n",
    "        symbol = \"\"\n",
    "\n",
    "    agent = DDDQNAgent(state_size=state_size,\n",
    "                     action_size=action_size,\n",
    "                     memory_size=memory_size,\n",
    "                     episodes=episodes,\n",
    "                     episode_length=episode_length,\n",
    "                     train_interval=train_interval,\n",
    "                     gamma=gamma,\n",
    "                     learning_rate=learning_rate,\n",
    "                     batch_size=batch_size,\n",
    "                     epsilon_min=epsilon_min,\n",
    "                     train_test=train_test,\n",
    "                     symbol=symbol)\n",
    "\n",
    "    # Warming up the agent\n",
    "    if (train_test == 'train'):\n",
    "        for _ in range(memory_size):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            agent.observe(state, action, reward, next_state, done, warming_up=True)\n",
    "        if display:\n",
    "            print('completed mem allocation: ', time.time() - start)\n",
    "\n",
    "    # Training the agent\n",
    "    loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    reward_list=[]\n",
    "    epsilon_list=[]\n",
    "    metrics_df=None\n",
    "    if train_test == \"train\":\n",
    "        best_loss = 9999\n",
    "        best_reward = 0\n",
    "        for ep in range(episodes):\n",
    "            ms = time.time()\n",
    "            state = environment.reset()\n",
    "            rew = 0\n",
    "            loss_list_temp = []\n",
    "            val_loss_list_temp = []\n",
    "\n",
    "            for _ in range(episode_length):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = environment.step(action)\n",
    "                loss = agent.observe(state, action, reward, next_state,\n",
    "                                     done)  # loss would be none if the episode length is not % by 10\n",
    "                state = next_state\n",
    "                rew += reward\n",
    "                if(loss):\n",
    "                    loss_list_temp.append(round(loss.history[\"loss\"][0],3))\n",
    "                    val_loss_list_temp.append(round(loss.history[\"val_loss\"][0],3))\n",
    "\n",
    "            if display:\n",
    "                print(\"Ep:\" + str(ep)\n",
    "                      + \"| rew:\" + str(round(rew, 2))\n",
    "                      + \"| eps:\" + str(round(agent.epsilon, 2))\n",
    "                      + \"| loss:\" + str(round(loss.history[\"loss\"][0], 4))\n",
    "                      + \"| runtime:\" + str(time.time() - ms))\n",
    "                print(\"Loss=\", str(np.mean(loss_list_temp)), \" Val_Loss=\", str(np.mean(val_loss_list_temp)))\n",
    "\n",
    "            loss_list.append(np.mean(loss_list_temp))\n",
    "            val_loss_list.append(np.mean(val_loss_list_temp))\n",
    "            reward_list.append(rew)\n",
    "            epsilon_list.append(round(agent.epsilon, 2))\n",
    "\n",
    "        agent.save_model()\n",
    "\n",
    "        metrics_df=pd.DataFrame({'loss':loss_list,'val_loss':val_loss_list,'reward':reward_list,'epsilon':epsilon_list})\n",
    "\n",
    "        if save_results:\n",
    "            metrics_df.to_csv('./Results/perf_metrics.csv')\n",
    "\n",
    "    if(train_test=='test'):\n",
    "        agent.load_model()\n",
    "\n",
    "    generator = TAStreamer(filename=filename, mode='test', split=train_test_split)\n",
    "    environment = Indicator_1(data_generator=generator,\n",
    "                              trading_fee=trading_fee,\n",
    "                              time_fee=time_fee,\n",
    "                              episode_length=episode_length,)\n",
    "\n",
    "    done = False\n",
    "    state = environment.reset()\n",
    "    q_values_list=[]\n",
    "    state_list=[]\n",
    "    action_list=[]\n",
    "    reward_list=[]\n",
    "    trade_list=[]\n",
    "\n",
    "    while not done:\n",
    "        action, q_values = agent.act(state, test=True)\n",
    "        state, reward, done, info = environment.step(action)\n",
    "        if 'status' in info and info['status'] == 'Closed plot':\n",
    "            done = True\n",
    "        else:\n",
    "            reward_list.append(reward)\n",
    "\n",
    "            calc_returns=environment.return_calc(render_show)\n",
    "            if calc_returns:\n",
    "                trade_list.append(calc_returns)\n",
    "\n",
    "            if(render_show):\n",
    "                environment.render()\n",
    "\n",
    "\n",
    "        q_values_list.append(q_values)\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "\n",
    "    print('Reward = %.2f' % sum(reward_list))\n",
    "\n",
    "    trades_df=pd.DataFrame(trade_list)\n",
    "    action_policy_df = pd.DataFrame({'q_values':q_values_list,'state':state_list,'action':action_list})\n",
    "\n",
    "    if save_results:\n",
    "        trades_df.to_csv(r'./Results/trade_list.csv')\n",
    "        action_policy_df.to_pickle(r'./Results/action_policy.pkl')\n",
    "\n",
    "    if display:\n",
    "        print(\"All done:\", str(time.time() - start))\n",
    "\n",
    "    return({\"metrics_df\":metrics_df,\n",
    "            \"trades_df\":trades_df,\n",
    "            \"action_policy_df\":action_policy_df,\n",
    "            \"reward_list\":reward_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stocks_csv=glob.glob(r\"./Data/*.csv\")\n",
    "stocks_csv=glob.glob(r\"./Data/aprova/*.csv\")\n",
    "stocks_csv.sort()\n",
    "len(stocks_csv)\n",
    "#symbol_path=dict(zip([re.findall(r'Data\\\\([^_]+)',x)[0] for x in stocks_csv],stocks_csv))\n",
    "# stocks_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "./Data/aprova/prova.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klop/.local/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'as_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-146798fc6d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time Taken = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8a651a2031b7>\u001b[0m in \u001b[0;36mWorld\u001b[0;34m(filename, train_test, episodes, train_test_split, trading_fee, time_fee, memory_size, gamma, epsilon_min, batch_size, train_interval, learning_rate, render_show, display, save_results)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTAStreamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/trading/Deep-Reinforcement-Learning-in-Trading-master/Environment/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **gen_kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# We pass self explicitely since we sometimes override rewind (see csv generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mDataGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mDataGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/trading/Deep-Reinforcement-Learning-in-Trading-master/Environment/core.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/trading/Deep-Reinforcement-Learning-in-Trading-master/Environment/gens/TA_Gen.py\u001b[0m in \u001b[0;36m_generator\u001b[0;34m(filename, header, split, mode, spread)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'as_matrix'"
     ]
    }
   ],
   "source": [
    "train_test_split=0.75\n",
    "results_dict={}\n",
    "\n",
    "for i,stocks in enumerate(stocks_csv):\n",
    "    print(i)\n",
    "    print(stocks)\n",
    "    start = time.time()\n",
    "    results=World(filename=stocks,train_test_split=train_test_split,episodes=100)\n",
    "    end = time.time()\n",
    "    print('Time Taken = ',end-start)\n",
    "    df=pd.read_csv(stocks)\n",
    "    df=df.iloc[int(train_test_split*len(df)):,:]\n",
    "    returns=df.close.pct_change()\n",
    "    perf_metrics = sharpe_calc(results['trades_df'].copy())\n",
    "\n",
    "    results['buy_and_hold_sharpe']=np.mean(returns)/np.std(returns)\n",
    "    results['strategy_sharpe']=perf_metrics['strategy_sharpe']\n",
    "    results['num_trades']=perf_metrics['num_trades']\n",
    "    results['position_df']=perf_metrics['position_df']\n",
    "\n",
    "    results_dict[df.Name.iloc[0]]=results\n",
    "    gc.collect()\n",
    "\n",
    "pickle.dump(results_dict, open(\"./Results/results_revised.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = pickle.load(open(r\"./Results/results_revised.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare with others\n",
    "results_df=pd.DataFrame({'Symbols':list(results_dict.keys()),\n",
    "'Buy and Hold Sharpe':[x['buy_and_hold_sharpe'] for x in list(results_dict.values())],\n",
    "'Strategy Sharpe':[x['strategy_sharpe'] for x in list(results_dict.values())],\n",
    "'Number of Trades':[x['num_trades'] for x in list(results_dict.values())],\n",
    "'Buy and Hold Total Return':[x['buy_n_hold_tot_return'] for x in list(results_dict.values())],\n",
    "'Strategy Total Return':[x['strategy_tot_return'] for x in list(results_dict.values())],\n",
    "})\n",
    "results_df_filt=results_df.dropna()\n",
    "results_df_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "results_df_filt.to_csv(\"Trade Analytics Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get chart \n",
    "test=results_df_filt[results_df_filt['Number of Trades']>2]\n",
    "x0 = np.random.randn(500)\n",
    "x1 = np.random.randn(500)+1\n",
    "\n",
    "trace1 = Histogram(\n",
    "    x=test[\"Strategy Sharpe\"],\n",
    "    opacity=0.75,\n",
    "    name='Buy_n_Hold'\n",
    ")\n",
    "trace2 = Histogram(\n",
    "    x=test['Buy and Hold Sharpe'],\n",
    "    opacity=0.75,\n",
    "    name='DDDQN'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = Layout(title='Risk Adjusted return Histogram',barmode='overlay')\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='overlaid histogram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
